{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 \n",
    "## 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. OR 데이터 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 퍼셉트론의 매개 변수 [[2. 2.]] [-1.]\n",
      "훈련집합에 대한 예측 [-1  1  1  1]\n",
      "정확률 측정 :  100.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# 훈련 집합 구축\n",
    "x = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y = [-1,1,1,1]\n",
    "\n",
    "# fit 함수로 Perceptron 학습\n",
    "p = Perceptron()\n",
    "p.fit(x,y)\n",
    "\n",
    "print(\"학습된 퍼셉트론의 매개 변수\",p.coef_,p.intercept_)\n",
    "print(\"훈련집합에 대한 예측\",p.predict(x))\n",
    "print(\"정확률 측정 : \",p.score(x,y)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 필기 숫자 데이터 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[77.  1.  0.  1.  0.  0.  2.  0.  0.  0.]\n",
      " [ 0. 65.  2.  2.  8.  1.  0.  0. 14.  1.]\n",
      " [ 0.  0. 73.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. 58.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. 64.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  3.  0. 63.  0.  0.  1.  2.]\n",
      " [ 0.  1.  0.  0.  1.  1. 78.  0.  8.  0.]\n",
      " [ 0.  0.  0.  2.  1.  1.  0. 61.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0. 52.  0.]\n",
      " [ 0.  0.  0.  2.  1.  2.  0.  1.  0. 65.]]\n",
      "테스트 집합에 대한 정확률 =  91.23783031988873 % 입니다.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "digit = datasets.load_digits()\n",
    "x_train,x_test,y_train,y_test = train_test_split(digit.data,digit.target,train_size=0.6)\n",
    "\n",
    "# fit 함수로 Perceptron 학습\n",
    "p = Perceptron(max_iter=100,eta0=0.001,verbose=0)\n",
    "p.fit(x_train,y_train) #digit 테이터로 모벨링\n",
    "\n",
    "res = p.predict(x_test) # 테스트 집합으로 예측\n",
    "\n",
    "# 혼동 행렬\n",
    "conf = np.zeros((10,10))\n",
    "\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "# 정확률 계산\n",
    "no_correct = 0\n",
    "for i in range(10):\n",
    "    no_correct += conf[i][i]\n",
    "accuracy = no_correct/len(res)\n",
    "print(\"테스트 집합에 대한 정확률 = \",accuracy*100,\"% 입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn 의 필기 숫자 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.40106973\n",
      "Iteration 2, loss = 0.33804648\n",
      "Iteration 3, loss = 0.21880686\n",
      "Iteration 4, loss = 0.16270003\n",
      "Iteration 5, loss = 0.13463516\n",
      "Iteration 6, loss = 0.11396073\n",
      "Iteration 7, loss = 0.10043825\n",
      "Iteration 8, loss = 0.08871727\n",
      "Iteration 9, loss = 0.08036757\n",
      "Iteration 10, loss = 0.07125316\n",
      "Iteration 11, loss = 0.06510181\n",
      "Iteration 12, loss = 0.05999661\n",
      "Iteration 13, loss = 0.05758024\n",
      "Iteration 14, loss = 0.05253436\n",
      "Iteration 15, loss = 0.04735516\n",
      "Iteration 16, loss = 0.04316002\n",
      "Iteration 17, loss = 0.04025942\n",
      "Iteration 18, loss = 0.03939235\n",
      "Iteration 19, loss = 0.03523740\n",
      "Iteration 20, loss = 0.03324584\n",
      "Iteration 21, loss = 0.03102449\n",
      "Iteration 22, loss = 0.02981767\n",
      "Iteration 23, loss = 0.02795935\n",
      "Iteration 24, loss = 0.02677000\n",
      "Iteration 25, loss = 0.02622434\n",
      "Iteration 26, loss = 0.02411722\n",
      "Iteration 27, loss = 0.02347740\n",
      "Iteration 28, loss = 0.02235018\n",
      "Iteration 29, loss = 0.02207299\n",
      "Iteration 30, loss = 0.02072914\n",
      "Iteration 31, loss = 0.01964497\n",
      "Iteration 32, loss = 0.01937740\n",
      "Iteration 33, loss = 0.01791460\n",
      "Iteration 34, loss = 0.01770550\n",
      "Iteration 35, loss = 0.01689766\n",
      "Iteration 36, loss = 0.01648623\n",
      "Iteration 37, loss = 0.01617571\n",
      "Iteration 38, loss = 0.01628221\n",
      "Iteration 39, loss = 0.01483370\n",
      "Iteration 40, loss = 0.01479686\n",
      "Iteration 41, loss = 0.01386572\n",
      "Iteration 42, loss = 0.01384631\n",
      "Iteration 43, loss = 0.01318935\n",
      "Iteration 44, loss = 0.01276497\n",
      "Iteration 45, loss = 0.01247802\n",
      "Iteration 46, loss = 0.01242450\n",
      "Iteration 47, loss = 0.01191432\n",
      "Iteration 48, loss = 0.01167519\n",
      "Iteration 49, loss = 0.01144990\n",
      "Iteration 50, loss = 0.01125180\n",
      "Iteration 51, loss = 0.01069547\n",
      "Iteration 52, loss = 0.01082437\n",
      "Iteration 53, loss = 0.01096860\n",
      "Iteration 54, loss = 0.01014133\n",
      "Iteration 55, loss = 0.01001248\n",
      "Iteration 56, loss = 0.00995830\n",
      "Iteration 57, loss = 0.00956275\n",
      "Iteration 58, loss = 0.00934273\n",
      "Iteration 59, loss = 0.00921189\n",
      "Iteration 60, loss = 0.00900367\n",
      "Iteration 61, loss = 0.00886342\n",
      "Iteration 62, loss = 0.00858135\n",
      "Iteration 63, loss = 0.00850839\n",
      "Iteration 64, loss = 0.00840120\n",
      "Iteration 65, loss = 0.00831352\n",
      "Iteration 66, loss = 0.00799094\n",
      "Iteration 67, loss = 0.00788604\n",
      "Iteration 68, loss = 0.00781369\n",
      "Iteration 69, loss = 0.00765735\n",
      "Iteration 70, loss = 0.00745284\n",
      "Iteration 71, loss = 0.00742805\n",
      "Iteration 72, loss = 0.00732370\n",
      "Iteration 73, loss = 0.00719832\n",
      "Iteration 74, loss = 0.00718713\n",
      "Iteration 75, loss = 0.00702843\n",
      "Iteration 76, loss = 0.00692150\n",
      "Iteration 77, loss = 0.00672466\n",
      "Iteration 78, loss = 0.00676772\n",
      "Iteration 79, loss = 0.00660127\n",
      "Iteration 80, loss = 0.00645065\n",
      "Iteration 81, loss = 0.00638072\n",
      "Iteration 82, loss = 0.00639922\n",
      "Iteration 83, loss = 0.00622191\n",
      "Iteration 84, loss = 0.00614523\n",
      "Iteration 85, loss = 0.00600916\n",
      "Iteration 86, loss = 0.00597484\n",
      "Iteration 87, loss = 0.00586238\n",
      "Iteration 88, loss = 0.00579785\n",
      "Iteration 89, loss = 0.00575476\n",
      "Iteration 90, loss = 0.00575682\n",
      "Iteration 91, loss = 0.00563355\n",
      "Iteration 92, loss = 0.00556718\n",
      "Iteration 93, loss = 0.00549006\n",
      "Iteration 94, loss = 0.00536461\n",
      "Iteration 95, loss = 0.00533150\n",
      "Iteration 96, loss = 0.00526972\n",
      "Iteration 97, loss = 0.00523863\n",
      "Iteration 98, loss = 0.00520525\n",
      "Iteration 99, loss = 0.00511236\n",
      "Iteration 100, loss = 0.00507012\n",
      "Iteration 101, loss = 0.00498747\n",
      "Iteration 102, loss = 0.00497887\n",
      "Iteration 103, loss = 0.00488938\n",
      "Iteration 104, loss = 0.00481860\n",
      "Iteration 105, loss = 0.00474981\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[[67.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0. 66.  0.  0.  0.  0.  1.  1.  3.  0.]\n",
      " [ 0.  1. 76.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0. 70.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. 65.  0.  0.  1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0. 76.  0.  0.  0.  2.]\n",
      " [ 0.  0.  0.  0.  0.  1. 74.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0. 68.  0.  0.]\n",
      " [ 0.  0.  0.  0.  2.  0.  0.  0. 66.  2.]\n",
      " [ 0.  1.  0.  0.  0.  3.  0.  0.  0. 68.]]\n",
      "테스트 집합에 대한 정확률은  96.80111265646731 % 입니다.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "digit = datasets.load_digits()\n",
    "x_train,x_test,y_train,y_test = train_test_split(digit.data,digit.target,train_size=0.6)\n",
    "\n",
    "#MPL 분류기 모델을 학습\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=32,max_iter=300,solver='sgd',verbose=True)\n",
    "mlp.fit(x_train,y_train)\n",
    "\n",
    "res = mlp.predict(x_test) # 테스트 집합으로 예측\n",
    "\n",
    "#혼동 행렬\n",
    "conf = np.zeros((10,10))\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "# 정확률 계산\n",
    "no_correct=0\n",
    "for i in range(10):\n",
    "    no_correct+=conf[i][i]\n",
    "accuracy = no_correct/len(res)\n",
    "print(\"테스트 집합에 대한 정확률은 \",accuracy*100,\"% 입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터셋으로 확장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60850820\n",
      "Iteration 2, loss = 0.27047413\n",
      "Iteration 3, loss = 0.21904079\n",
      "Iteration 4, loss = 0.18441040\n",
      "Iteration 5, loss = 0.15890209\n",
      "Iteration 6, loss = 0.13872661\n",
      "Iteration 7, loss = 0.12234523\n",
      "Iteration 8, loss = 0.10853985\n",
      "Iteration 9, loss = 0.09777905\n",
      "Iteration 10, loss = 0.08836896\n",
      "Iteration 11, loss = 0.08030759\n",
      "Iteration 12, loss = 0.07335521\n",
      "Iteration 13, loss = 0.06736680\n",
      "Iteration 14, loss = 0.06214992\n",
      "Iteration 15, loss = 0.05726335\n",
      "Iteration 16, loss = 0.05355107\n",
      "Iteration 17, loss = 0.04936703\n",
      "Iteration 18, loss = 0.04587349\n",
      "Iteration 19, loss = 0.04280114\n",
      "Iteration 20, loss = 0.03946514\n",
      "Iteration 21, loss = 0.03652675\n",
      "Iteration 22, loss = 0.03481748\n",
      "Iteration 23, loss = 0.03189138\n",
      "Iteration 24, loss = 0.03004875\n",
      "Iteration 25, loss = 0.02808042\n",
      "Iteration 26, loss = 0.02613918\n",
      "Iteration 27, loss = 0.02407635\n",
      "Iteration 28, loss = 0.02246562\n",
      "Iteration 29, loss = 0.02140110\n",
      "Iteration 30, loss = 0.01949004\n",
      "Iteration 31, loss = 0.01852007\n",
      "Iteration 32, loss = 0.01704720\n",
      "Iteration 33, loss = 0.01560367\n",
      "Iteration 34, loss = 0.01497286\n",
      "Iteration 35, loss = 0.01367816\n",
      "Iteration 36, loss = 0.01292394\n",
      "Iteration 37, loss = 0.01232672\n",
      "Iteration 38, loss = 0.01131919\n",
      "Iteration 39, loss = 0.01059793\n",
      "Iteration 40, loss = 0.01006645\n",
      "Iteration 41, loss = 0.00906077\n",
      "Iteration 42, loss = 0.00811111\n",
      "Iteration 43, loss = 0.00784755\n",
      "Iteration 44, loss = 0.00706978\n",
      "Iteration 45, loss = 0.00675202\n",
      "Iteration 46, loss = 0.00627027\n",
      "Iteration 47, loss = 0.00606847\n",
      "Iteration 48, loss = 0.00536244\n",
      "Iteration 49, loss = 0.00497308\n",
      "Iteration 50, loss = 0.00474389\n",
      "Iteration 51, loss = 0.00453292\n",
      "Iteration 52, loss = 0.00406137\n",
      "Iteration 53, loss = 0.00382457\n",
      "Iteration 54, loss = 0.00381845\n",
      "Iteration 55, loss = 0.00336922\n",
      "Iteration 56, loss = 0.00316233\n",
      "Iteration 57, loss = 0.00297955\n",
      "Iteration 58, loss = 0.00269781\n",
      "Iteration 59, loss = 0.00259588\n",
      "Iteration 60, loss = 0.00263748\n",
      "Iteration 61, loss = 0.00227491\n",
      "Iteration 62, loss = 0.00215540\n",
      "Iteration 63, loss = 0.00208560\n",
      "Iteration 64, loss = 0.00192526\n",
      "Iteration 65, loss = 0.00177162\n",
      "Iteration 66, loss = 0.00175357\n",
      "Iteration 67, loss = 0.00168848\n",
      "Iteration 68, loss = 0.00154691\n",
      "Iteration 69, loss = 0.00141888\n",
      "Iteration 70, loss = 0.00136317\n",
      "Iteration 71, loss = 0.00130905\n",
      "Iteration 72, loss = 0.00126343\n",
      "Iteration 73, loss = 0.00126456\n",
      "Iteration 74, loss = 0.00114579\n",
      "Iteration 75, loss = 0.00105719\n",
      "Iteration 76, loss = 0.00101280\n",
      "Iteration 77, loss = 0.00099227\n",
      "Iteration 78, loss = 0.00095131\n",
      "Iteration 79, loss = 0.00086310\n",
      "Iteration 80, loss = 0.00084759\n",
      "Iteration 81, loss = 0.00083917\n",
      "Iteration 82, loss = 0.00085174\n",
      "Iteration 83, loss = 0.00077698\n",
      "Iteration 84, loss = 0.00070286\n",
      "Iteration 85, loss = 0.00068071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[[ 971    0    4    0    4    2    7    1    6    3]\n",
      " [   0 1124    2    0    0    0    2    5    0    2]\n",
      " [   1    3 1006    4    3    0    3    8    4    0]\n",
      " [   1    1    4  990    0   10    1    3    5    3]\n",
      " [   1    0    1    0  959    0    3    1    5    8]\n",
      " [   1    1    0    3    0  869    5    0    4    2]\n",
      " [   1    2    2    0    3    2  935    0    1    0]\n",
      " [   1    1    7    3    2    1    1 1005    4    3]\n",
      " [   2    3    5    2    2    6    1    1  941    5]\n",
      " [   1    0    1    8    9    2    0    4    4  983]]\n",
      "테스트 집합에 대한 정확률은 97.83 % 입니다.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# MNIST 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "mnist = fetch_openml('mnist_784')\n",
    "mnist.data = mnist.data/255.0\n",
    "x_train = mnist.data[:60000]; x_test=mnist.data[60000:]\n",
    "y_train = np.int16(mnist.target[:60000]); y_test = np.int16(mnist.target[60000:])\n",
    "\n",
    "#MPL 분류기 모델을 학습\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=512,max_iter=300,solver='adam',verbose=True)\n",
    "mlp.fit(x_train,y_train)\n",
    "\n",
    "# 테스트 집합으로 예측\n",
    "res = mlp.predict(x_test)\n",
    "\n",
    "#혼동 행렬\n",
    "conf = np.zeros((10,10),dtype=np.int16)\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "# 정확률 계산\n",
    "no_correct=0\n",
    "for i in range(10):\n",
    "    no_correct+=conf[i][i]\n",
    "accuracy = no_correct/len(res)\n",
    "print(\"테스트 집합에 대한 정확률은\",accuracy*100,\"% 입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 은닉 노드 개수 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter hidden_layer_size for estimator MLPClassifier(batch_size=32, max_iter=300, solver='sqd'). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 222, in __call__\n    return self.function(*args, **kwargs)\n  File \"c:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 586, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"c:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 230, in set_params\n    raise ValueError('Invalid parameter %s for estimator %s. '\nValueError: Invalid parameter hidden_layer_size for estimator MLPClassifier(batch_size=32, max_iter=300, solver='sqd'). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13036/1155127245.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sqd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrain_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_score\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mvalidation_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"hidden_layer_size\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprange\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"하이퍼 매개변수 최적화에 걸린 시간은\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstrat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"초 입니다.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mvalidation_curve\u001b[1;34m(estimator, X, y, param_name, param_range, groups, cv, scoring, n_jobs, pre_dispatch, verbose, error_score, fit_params)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n\u001b[0;32m   1648\u001b[0m                         verbose=verbose)\n\u001b[1;32m-> 1649\u001b[1;33m     results = parallel(delayed(_fit_and_score)(\n\u001b[0m\u001b[0;32m   1650\u001b[0m         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xhfl0\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xhfl0\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xhfl0\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                 \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter hidden_layer_size for estimator MLPClassifier(batch_size=32, max_iter=300, solver='sqd'). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split,validation_curve\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#\n",
    "digit = datasets.load_digits()\n",
    "x_train,x_test,y_train,y_test = train_test_split(digit.data,digit.target,train_size=0.6)\n",
    "\n",
    "\n",
    "#\n",
    "strat = time.time()\n",
    "mlp = MLPClassifier(learning_rate_init=0.001,batch_size=32,max_iter=300,solver='sqd')\n",
    "prange = range(50,1001,50)\n",
    "train_score,test_score =validation_curve(mlp,x_train,y_train,param_name=\"hidden_layer_size\",param_range=prange,cv = 10,scoring=\"accuracy\",n_jobs=4)\n",
    "end = time.time()\n",
    "print(\"하이퍼 매개변수 최적화에 걸린 시간은\",end-strat,\"초 입니다.\")\n",
    "\n",
    "#\n",
    "train_mean = np.mean(train_score,axis=1)\n",
    "train_std = np.std(train_score,axis=1)\n",
    "test_mean = np.mean(test_score,axis=1)\n",
    "test_std = np.std(test_score,axis=1)\n",
    "\n",
    "#\n",
    "plt.plot(prange,train_mean,label=\"Train score\",color = \"r\")\n",
    "plt.plot(prange,test_mean,label=\"Test score\",color=\"b\")\n",
    "plt.fill_between(prange,train_mean-train_std,train_mean+train_std,alpha=0.2,color=\"r\")\n",
    "plt.fill_between(prange,test_mean-test_std,test_mean+test_std,alpha=0.2,color=\"b\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Validation Curve with MLP\")\n",
    "plt.xlabel(\"Number of hidden nodes\");plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.9,1.01)\n",
    "plt.grid(axis='both')\n",
    "plt.show()\n",
    "\n",
    "best_number_nodes = prange[np.argmax(test_mean)]\n",
    "print(\"\\n 최적의 노드 개수\",best_number_nodes,\" 개 입니다.\\n\")\n",
    "\n",
    "#\n",
    "mlp_test = MLPClassifier(hidden_layer_sizes=(best_number_nodes),learning_rate_init=0.001,batch_size=32,max_iter=300,solver='sgd')\n",
    "mlp_test.fit(x_train,y_train)\n",
    "\n",
    "#\n",
    "res = mlp_test.predict(x_test)\n",
    "\n",
    "#\n",
    "conf = np.zeros((10,10))\n",
    "for i in range(len(res)):\n",
    "    conf[res[i][y_test[i]]]+=1\n",
    "print(conf)\n",
    "\n",
    "#\n",
    "no_correct = 0\n",
    "for i in range(10):\n",
    "    no_correct+=conf[i][i]\n",
    "accuracy = no_correct/len(res)\n",
    "print(\"정확률은\",accuracy*100,\" 입니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8bba640f0286e654255f2d7620100003c0b313612cdde4bf95181db9db66947f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
